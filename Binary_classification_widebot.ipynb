{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary classification_widebot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tn4tKwJXPNv",
        "colab_type": "text"
      },
      "source": [
        "# **Binary Classification on Widebot Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5voavUiUk61s",
        "colab_type": "text"
      },
      "source": [
        "# *Download Needed Dependencies:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62gYtY0JTtIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e3354dc4-b7bd-4c8c-d093-c926aee98bbd"
      },
      "source": [
        "!pip install category_encoders"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.0.5)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH-7i8h-l8ib",
        "colab_type": "text"
      },
      "source": [
        "# Get Access to Google drive through Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rwYEqh3hUFg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e04fa1e-9604-41e5-fe78-e2542e1ebfce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u06WZF8knv44",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbMvTeJ0jY7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "from numpy import genfromtxt\n",
        "import numpy as np\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import copy\n",
        "import seaborn as sns\n",
        "import sklearn as sk\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import KFold,cross_val_score\n",
        "from sklearn import preprocessing,tree,svm\n",
        "from sklearn.metrics import confusion_matrix, classification_report \n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss,NeighbourhoodCleaningRule,CondensedNearestNeighbour,OneSidedSelection,EditedNearestNeighbours,TomekLinks\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import category_encoders as ce\n",
        "import warnings\n",
        "import random\n",
        "from sklearn.preprocessing import normalize\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers  import Dense,Dropout\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEjM-D3boEz6",
        "colab_type": "text"
      },
      "source": [
        "# Data Analysis :\n",
        "\n",
        "After reviewing data, It's noticed that:\n",
        "\n",
        "- The data contains numerical and categorical features. the unique values:\n",
        "    - feature 1 -> a, b\n",
        "    - feature 2, 3, 8, 14, 15, 17 -> floats\n",
        "    - feature 4 -> u, y, l\n",
        "    - feature 5 -> g, p, gg\n",
        "    - feature 6 -> c, k, ff, i, j, q, W, d, m, cc, aa, r, x, e\n",
        "    - feature 7 -> v, ff, o, h, j, bb, n, z, dd\n",
        "    - feature 9, 10, 18 -> f, t\n",
        "    - feature 11, 19 -> 0, 1\n",
        "    - feature 12 -> f, t\n",
        "    - feature 13 -> g, s, p\n",
        "    - classLabel -> yes, no\n",
        "- The data contains missing values in both the numerical and categorical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2_YThRvcWP1",
        "colab_type": "text"
      },
      "source": [
        "## Blocks of the Algorithm :\n",
        "\n",
        "- Calculate correlation between data to determine the linearly mapped features as it will make the data modeling biased.\n",
        "\n",
        "- Handling Missing Data for categorical and numerical features.\n",
        "\n",
        "- Transform the categorical features into numerical values.\n",
        "\n",
        "- Use Machine Learning to model the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCIUDuLefKgm",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Correlation Calculation :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzO0gSSbl-qm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_replace_map (csv_reader,cat_csv_reader):\n",
        "    replace_map={}\n",
        "    for name in cat_csv_reader.columns:\n",
        "        labels = cat_csv_reader[name].astype('category').cat.categories.tolist()\n",
        "        replace_map[name] = {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}\n",
        "    return replace_map\n",
        "def replace_cat_with_num(csv_reader,replace_map):\n",
        "    csv_reader.replace(replace_map, inplace=True)\n",
        "    return csv_reader"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb1sITwDs7wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correlation_enhancer(train_df,test_df,train_copy):\n",
        "    correlation_cof = train_copy.corr(method ='pearson')\n",
        "    vis = {}\n",
        "    cnt=0\n",
        "    print(\"pairs of highly correlated features :\")\n",
        "    for i in train_df.columns:\n",
        "        for j in train_df.columns:\n",
        "            if abs(correlation_cof[i][j])>=0.8 and i!=j:\n",
        "                if vis.get(i)==None and vis.get(j)==None and i!='variable18' and j!='variable18' :\n",
        "                    train_df.drop(i, axis=1,inplace=True)\n",
        "                    test_df.drop(i, axis=1,inplace=True)\n",
        "                    vis[i]=1\n",
        "                    print(i,\"  \",j, \"  \",correlation_cof[i][j])\n",
        "                    cnt+=1\n",
        "    print(\"number of features to be droped : \",cnt)\n",
        "    return train_df,test_df"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGslUw7_khCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0f4518ca-8edb-4b1a-c189-c6a1f3acf894"
      },
      "source": [
        "train_df = pd.read_csv('drive/My Drive/Data/training.csv', sep=';',decimal=',')\n",
        "test_df = pd.read_csv('drive/My Drive/Data/validation.csv', sep=';',decimal=',')\n",
        "train_copy = train_df.dropna()\n",
        "cat_train = train_df.select_dtypes(include=['object']).copy()\n",
        "rp = get_replace_map (train_copy,cat_train)\n",
        "train_copy = replace_cat_with_num(train_copy,rp)\n",
        "train_df,test_df = correlation_enhancer(train_df,test_df,train_copy)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pairs of highly correlated features :\n",
            "variable4    variable5    0.8319892083783643\n",
            "variable14    variable17    1.00000000000001\n",
            "variable19    classLabel    1.0\n",
            "number of features to be droped :  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toUXQjIQsHxL",
        "colab_type": "text"
      },
      "source": [
        "Analysis :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFWwgqEGgM4X",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Handline Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzLjY8AWJk22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_frequent_object(csv_reader,cat_csv_reader):\n",
        "    frequent_object={}\n",
        "    for name in cat_csv_reader.columns:\n",
        "        frequent_object[name] = csv_reader[name].value_counts().index[0]\n",
        "    return frequent_object\n",
        "def modify_nan_dtype_object(csv_reader,cat_csv_reader,frequent_object):\n",
        "    for name in cat_csv_reader.columns:\n",
        "        for indx,row in csv_reader.iterrows():\n",
        "            if pd.isna(csv_reader[name][indx]):\n",
        "                csv_reader[name][indx] = frequent_object[name]\n",
        "    return csv_reader\n",
        "def modift_nan_flaot_int(csv_reader,avg_csv_reader):\n",
        "    for name in csv_reader.columns:\n",
        "        if csv_reader[name].dtype=='object':\n",
        "            continue\n",
        "        for indx,row in csv_reader.iterrows():\n",
        "            if pd.isna(csv_reader[name][indx]):\n",
        "                csv_reader[name][indx] = avg_csv_reader[name]\n",
        "    return csv_reader"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzdx2SheghuI",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Transform Categorical Features into numerical values :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slhEuWUhSL8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_replace_map_binay_encoder (csv_reader_train,csv_reader_test,cat_csv_reader):\n",
        "    for name in cat_csv_reader.columns:\n",
        "        if name != 'classLabel':\n",
        "            encoder = ce.BinaryEncoder(cols=[name])\n",
        "            csv_reader_train = encoder.fit_transform(csv_reader_train)\n",
        "            csv_reader_test = encoder.transform(csv_reader_test)\n",
        "    labels = csv_reader_train['classLabel'].astype('category').cat.categories.tolist()\n",
        "    replace_map = {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}\n",
        "    csv_reader_train.replace(replace_map, inplace=True)\n",
        "    csv_reader_test.replace(replace_map, inplace=True)\n",
        "    return csv_reader_train,csv_reader_test"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K27AmCnJi3ul",
        "colab_type": "text"
      },
      "source": [
        "## 4 - Process train and test Data :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHcGVkhu-JB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_data(csv_reader):\n",
        "    csv_reader=csv_reader.drop('variable18', axis=1)\n",
        "    #sns.catplot(x=\"variable3\", y=\"classLabel\", data=csv_reader);\n",
        "    # handling missing data for categorical Variable\n",
        "    cat_csv_reader = csv_reader.select_dtypes(include=['object']).copy()\n",
        "    frequent_object = get_frequent_object(csv_reader,cat_csv_reader)\n",
        "    csv_reader = modify_nan_dtype_object(csv_reader,cat_csv_reader,frequent_object)\n",
        "    # handling missing data for numerical Variable\n",
        "    int_float_modifier = csv_reader.mean(axis = 0)\n",
        "    csv_reader = modift_nan_flaot_int(csv_reader,int_float_modifier)\n",
        "    return csv_reader,int_float_modifier,frequent_object,cat_csv_reader"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAjv_L_o-UJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_data(csv_reader,int_float_modifier,frequent_object):\n",
        "    csv_reader=csv_reader.drop('variable18', axis=1)\n",
        "    # handling missing data for categorical Variable\n",
        "    cat_csv_reader = csv_reader.select_dtypes(include=['object']).copy()\n",
        "    csv_reader = modify_nan_dtype_object(csv_reader,cat_csv_reader,frequent_object)\n",
        "    # handling missing data for numerical Variable\n",
        "    csv_reader = modift_nan_flaot_int(csv_reader,int_float_modifier)\n",
        "    return csv_reader"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aueps-yhXNZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataFrame,int_float_modifier,frequent_object,cat_csv_reader = get_train_data(train_df)\n",
        "test_dataFrame = get_test_data(test_df,int_float_modifier,frequent_object)\n",
        "train_dataFrame,test_dataFrame = get_replace_map_binay_encoder (train_dataFrame,test_dataFrame,cat_csv_reader)"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1gydJYijzfq",
        "colab_type": "text"
      },
      "source": [
        "## *5* - Normalize the data :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is63fszjNcp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def normalize_data(train_dataFrame,test_dataFrame,indx):\n",
        "indx = [1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0]\n",
        "y_train = train_dataFrame[\"classLabel\"].values\n",
        "y_test = test_dataFrame[\"classLabel\"].values\n",
        "train_data_all =  train_dataFrame.values \n",
        "test_data_all = test_dataFrame.values\n",
        "x_mean = train_dataFrame.mean()\n",
        "x_std = train_dataFrame.std()\n",
        "for i in range(0,train_data_all.shape[1]):\n",
        "    if (indx[i]==0):\n",
        "        train_data_all[:,i] = (train_data_all[:,i] - x_mean[i])/ (x_std[i]*x_std[i] + 0.000000001)\n",
        "        train_data_all[:,i] = (train_data_all[:,i] - x_mean[i])/ (x_std[i]*x_std[i] + 0.000000001)\n",
        "\n",
        "train_data_all =  train_dataFrame.values \n",
        "test_data_all = test_dataFrame.values\n",
        "x_train = train_data_all[:,:-1]\n",
        "x_test = test_data_all[:,:-1]"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNPejOgMhL2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.fit_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY0XLMMzhMNt",
        "colab_type": "text"
      },
      "source": [
        "## *6* - Use Machine Learning to model the data :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lXMMHAthlQd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "faf8683f-c62e-4507-a31e-310f0f65bdc7"
      },
      "source": [
        "suffled_indx = np.arange(x_train.shape[0])\n",
        "random.shuffle(suffled_indx)\n",
        "x_train=x_train[suffled_indx]\n",
        "y_train=y_train[suffled_indx]\n",
        "\n",
        "print(\"shape of train data : \",x_train.shape,\" shape of test : \",x_test.shape)\n",
        "print(\"ones equal \",sum(y_train == 1))\n",
        "print(\"zeros equal \",sum(y_train == 0))\n",
        "\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(max_depth=2) #max_depth=5\n",
        "print()\n",
        "clf = clf.fit(x_train, y_train.ravel())\n",
        "print(\"tree\")\n",
        "print(clf.score(x_train, y_train.ravel()))\n",
        "print(clf.score(x_test, y_test.ravel()))\n",
        "predictions = clf.predict(x_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "clf3 = RandomForestClassifier() #max_depth=5\n",
        "clf3 = clf3.fit(x_train, y_train.ravel())\n",
        "print(\"random forest\")\n",
        "print(clf3.score(x_train, y_train.ravel()))\n",
        "print(clf3.score(x_test, y_test.ravel()))\n",
        "predictions = clf3.predict(x_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "\n",
        "print(\"SVM\")\n",
        "clf2 = svm.SVC(kernel='poly',degree=2,max_iter=50).fit(x_train, y_train) #kernel='rbf', C=3\n",
        "print(clf2.score(x_train, y_train.ravel()))\n",
        "print(clf2.score(x_test, y_test.ravel()))\n",
        "predictions = clf2.predict(x_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "print(\"Logestic Regression\")\n",
        "lr2 = LogisticRegression(max_iter=200) \n",
        "lr2.fit(x_train, y_train.ravel()) \n",
        "print(lr2.score(x_train, y_train.ravel()))\n",
        "print(lr2.score(x_test, y_test.ravel()))\n",
        "predictions = lr2.predict(x_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "print(\"xgboost\")\n",
        "xg = XGBClassifier(max_iter=50)\n",
        "xg.fit(x_train, y_train)\n",
        "print(xg.score(x_train, y_train.ravel()))\n",
        "print(xg.score(x_test, y_test.ravel()))\n",
        "predictions = xg.predict(x_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "print(\"Keras\")\n",
        "model = Sequential()\n",
        "# For first layer, input shape must be supplied\n",
        "layer1 = Dense(units = 300, activation = 'relu', input_dim = x_train.shape[1])\n",
        "model.add(layer1)\n",
        "model.add(Dropout(0.50))\n",
        "\n",
        "layer2 = Dense(units = 1000, activation = 'relu')\n",
        "model.add(layer2)\n",
        "\n",
        "layer3 = Dense(units = 1000, activation = 'relu')\n",
        "model.add(layer3)\n",
        "layer5 = Dense(units = 500, activation = 'relu')\n",
        "model.add(layer5)\n",
        "\n",
        "layer4 = Dense(units = 1, activation = 'sigmoid')\n",
        "model.add(layer4)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "nn_score = model.evaluate(x_test, y_test)[1]\n",
        "print(nn_score)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of train data :  (3700, 30)  shape of test :  (200, 30)\n",
            "ones equal  [3424]\n",
            "zeros equal  [276]\n",
            "\n",
            "tree\n",
            "0.9454054054054054\n",
            "0.79\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.63      0.76       107\n",
            "           1       0.69      0.98      0.81        93\n",
            "\n",
            "    accuracy                           0.79       200\n",
            "   macro avg       0.83      0.80      0.79       200\n",
            "weighted avg       0.84      0.79      0.79       200\n",
            "\n",
            "random forest\n",
            "1.0\n",
            "0.84\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.76      0.84       107\n",
            "           1       0.77      0.94      0.84        93\n",
            "\n",
            "    accuracy                           0.84       200\n",
            "   macro avg       0.85      0.85      0.84       200\n",
            "weighted avg       0.86      0.84      0.84       200\n",
            "\n",
            "SVM\n",
            "0.9256756756756757\n",
            "0.47\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.01      0.02       107\n",
            "           1       0.47      1.00      0.64        93\n",
            "\n",
            "    accuracy                           0.47       200\n",
            "   macro avg       0.73      0.50      0.33       200\n",
            "weighted avg       0.75      0.47      0.31       200\n",
            "\n",
            "Logestic Regression\n",
            "0.9254054054054054\n",
            "0.465\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       107\n",
            "           1       0.47      1.00      0.63        93\n",
            "\n",
            "    accuracy                           0.47       200\n",
            "   macro avg       0.23      0.50      0.32       200\n",
            "weighted avg       0.22      0.47      0.30       200\n",
            "\n",
            "xgboost\n",
            "0.9840540540540541\n",
            "0.8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.65      0.78       107\n",
            "           1       0.71      0.97      0.82        93\n",
            "\n",
            "    accuracy                           0.80       200\n",
            "   macro avg       0.83      0.81      0.80       200\n",
            "weighted avg       0.84      0.80      0.80       200\n",
            "\n",
            "Keras\n",
            "Epoch 1/10\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 6403.3701 - accuracy: 0.8703\n",
            "Epoch 2/10\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 436.7144 - accuracy: 0.8692\n",
            "Epoch 3/10\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 243.0631 - accuracy: 0.8705\n",
            "Epoch 4/10\n",
            "116/116 [==============================] - 1s 6ms/step - loss: 136.2550 - accuracy: 0.8716\n",
            "Epoch 5/10\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 89.2683 - accuracy: 0.8676\n",
            "Epoch 6/10\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 77.2913 - accuracy: 0.8714\n",
            "Epoch 7/10\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 51.9714 - accuracy: 0.8730\n",
            "Epoch 8/10\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 36.4202 - accuracy: 0.8765\n",
            "Epoch 9/10\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 33.1217 - accuracy: 0.8705\n",
            "Epoch 10/10\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 27.6745 - accuracy: 0.8668\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 12.6196 - accuracy: 0.4650\n",
            "0.4650000035762787\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}